{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Brief intro to ML\n",
    "\n",
    "Machine learning emerged from inquiries in statistics, computer science, information theory, artificial intelligence, and pattern recognition. We can think of it as sets of tools for investigating, modeling, and understanding data. \n",
    "\n",
    "[Data splitting](https://www.mff.cuni.cz/veda/konference/wds/proc/pdf10/WDS10_105_i1_Reitermanova.pdf) is a fundamental preprocessing step used to divide a dataset into \"training\" and \"test\" sets. The majority portion of the data (say, 75%) is assigned to the training set, while the remaining 25% of data is assigned to the test set. Missing data should be handled before the splitting process commences.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[k-fold cross validation](https://en.wikipedia.org/wiki/Cross-validation_(statistics)) is preferred because it repeats this splitting process **k** number of times, where each fold  serves as the training set **k-1** times and as the test set exactly once. \n",
    "\n",
    "Thus, in 4-fold cross validation, the data are divided into four equal sized chunks. In the first iteration, the first 25% of the data is the test set, while the remaining 75% is the training set. In the second iteration, the second 25% is the test set and the other 75% is the training set, and so on:\n",
    "![cv](img/K-fold_cross_validation_EN.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised machine learning\n",
    "The syntax looks like this: **_Y ~ X_**\n",
    "\n",
    "_*Y*_ = dependent/target/outcome/response variable  \n",
    "_*X*_ = independent/predictor/input variable  \n",
    "\n",
    "In supervised machine learning, the training dataset is used to train a model that predicts the outputs of a target function, or an estimation of the actual but unknown function that maps X to Y. Here, the model learns the characteristics of the data and its performance is evaluated using a \"performance metric\" - a metric that describes how well it could predict the outcomes (accuracy, MSE, RMSE, AUC, etc.). This gives us a way to measure the goodness of fit for the model on the training data. \n",
    "\n",
    "However, we also want to see how well it performs on the test dataset, or data it has not yet seen! The model performs well if it has similar performance on the test dataset it has never seen before, because it can be generalized to new data. If the model performs poorly on the training dataset it is said to be **underfit** because it was not able to learn about relationships between the X and Y variables. If it performs well on the training set but poorly on the test set the model is said to be **overfit** because the model performed worse than expected when given new data (although patterns might be due to noise).  \n",
    "\n",
    "This can be extended to unsupervised learning as well, where there is no **_Y_** variable - the target function seeks to identify patterns in the data rather than predicting an outcome. \n",
    "\n",
    "\n",
    "# Classification or regression?\n",
    "\n",
    "**Classification** is used when the Y outcome variable is categorical. \"yes\" or \"no\" is a _binary_ example: 1 is prediction of the \"yes\" category and 0 as the \"no\". This can be extended to multi-level classification as well.  \n",
    "\n",
    "**Regression** is used when the Y outcome variable is continuous and we want the model to predict it using X variable(s). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The value of understanding simple linear regression - by hand! \n",
    "\n",
    "Doing a simple [OLS](https://en.wikipedia.org/wiki/Ordinary_least_squares) regression by hand provides a way to understand what supervised machine learning is doing under the hood. OLS regression builds a model that tries to predict Y using X (regresses Y onto X). \n",
    "\n",
    "First, let's generate toy predictor (X) and response (Y) variables and compute their means. This will be our \"training set\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate toy predictor (X) and response (Y) variables\n",
    "import numpy\n",
    "X = numpy.array([2, 4, 8, 12, 18, 20])\n",
    "Y = numpy.array([1, 3, 5, 9, 19, 21])\n",
    "\n",
    "## Calculate their means\n",
    "mean_X = round(sum(X) / len(X), 2)\n",
    "mean_Y = round(sum(Y) / len(Y), 2)\n",
    "print(\"mean of X is:\", mean_X)\n",
    "print(\"mean of Y is:\", mean_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge \n",
    "\n",
    "Draw a scatter plot with your X values on the x-axis and your Y values on the y-axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also perform \"vectorized\" operations, such as subtracting the mean of X from each X value or the mean of Y from each Y value. That is, we can do math on arrays of numbers simultaneously: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X-mean_X)\n",
    "print(Y-mean_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is important bcause it helps us calculate our [beta](https://en.wikipedia.org/wiki/Standardized_coefficient) coefficients in the regression. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Estimate the B1 coefficient (slope)\n",
    "B1 = sum((X-mean_X) * (Y-mean_Y)) / sum((X-mean_X)**2)\n",
    "print(\"slope is equal to\", round(B1,2))\n",
    "\n",
    "## Estimate B0 coefficient (intercept)\n",
    "B0 = mean_Y - (B1 * mean_X)\n",
    "print(\"intercept is equal to\", round(B0, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Plot the best fit line using the intercept and slope!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have calculated the OLS \"best fit line\" (the line that minimizes the sum of the squared errors) so that we can generate predicted values (our \"test set\") and assess the performance of the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Generate predicted Y values by plugging in our X values to the equation: \n",
    "Y_hat = B0 + B1 * X\n",
    "print(Y_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our performance metric will be [root mean square error (RMSE)](https://en.wikipedia.org/wiki/Root-mean-square_deviation), or the standard deviation of the residuals (aka prediction errors). Error is measured as the vertical distance from a data point to the best fit line. \n",
    "\n",
    "However, we have to do some calculations first before we get to RMSE! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## First, calculate the error for each observation by subracting the predicted value from it:\n",
    "Y_err = Y - Y_hat\n",
    "print(Y_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Second, calculate the square of each of these errors:\n",
    "Y_err_sq = Y_err**2\n",
    "print(Y_err_sq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Third, sum these values\n",
    "sum_squared_err = sum(Y_err_sq)\n",
    "print(sum_squared_err)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Fourth, calculate the RMSE - take the square root of the summed squared error divided by the length of Y:\n",
    "import math\n",
    "RMSE = math.sqrt(sum_squared_err / len(Y))\n",
    "print(round(RMSE, 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words model\n",
    "\n",
    "Before we can \"do\" machine learning, keep in mind you might want to perform preprocessing steps as outlined in notebook 1-4. Then, we need to get our text into numeric form so we can plug it into our machine learning models. \n",
    "\n",
    "A bag of words model classifies a text by turning it into a \"bag\" of words where words are normalized and counted. \n",
    "\n",
    "# CountVectorizer\n",
    "\n",
    "**`CountVectorizer`** will help us quickly tokenize text, learn its vocabulary, and encode the text as a vector for use in machine learning. This is often referred to as document encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!pip install -U numpy scipy scikit-learn\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = [\n",
    "    \"This is the first document.\",\n",
    "    \"This is the second second document.\",\n",
    "    \"And the third one.\",\n",
    "    \"Is this the first document?\"\n",
    "]\n",
    "# Define our vectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Use the .fit method to tokenize the text and learn the vocabulary\n",
    "vectorizer.fit(corpus)\n",
    "\n",
    "# Print the vocabulary\n",
    "print(vectorizer.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode the document\n",
    "vector = vectorizer.transform(corpus)\n",
    "print(vector) # 4 x 9 sparse matrix\n",
    "print(vector.shape)\n",
    "print(type(vector))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View the vectors as arrays (4 x 9)\n",
    "print(vector.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the arrays in the above cell. In which documents does \"and\" appear? What about \"document\"? What about \"the\"?\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# What does this tell us? \n",
    "vectorizer.transform(['this is']).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bigrams\n",
    "\n",
    "In addition to uni-grams, using bigrams can be useful to preserve some ordering information. \n",
    "\n",
    "> NOTE: **`ngram_range=(1,2)`** will get you bigrams, **`ngram_range=(1,3)`** will get you tri-grams, **`ngram_range=(1,4)`** will get you quad-grams, etc. \n",
    "\n",
    "> **`token_pattern=r'\\b\\w+\\b'`** is standard regex code to separate words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "bigram_vectorizer = CountVectorizer(ngram_range=(1, 2),\n",
    "                                    token_pattern=r'\\b\\w+\\b', min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyze = bigram_vectorizer.build_analyzer()\n",
    "analyze('Bi-grams are cool!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = bigram_vectorizer.fit_transform(corpus).toarray()\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_index = bigram_vectorizer.vocabulary_.get('this is')\n",
    "X[:, feature_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge\n",
    "\n",
    "Repeat this steps using the text below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# quote from Kathryn E. Piquette: http://dhdebates.gc.cuny.edu/debates/text/40\n",
    "DH = [\"The digital humanities are a 'community of practice'\",\n",
    "      \"(to borrow Etienne Wenger’s phrase)\", \n",
    "      \"whereby the learning, construction, and sharing of humanities knowledge\",\n",
    "      \"is undertaken with the application of digital technologies\",\n",
    "      \"in a reflexive, theoretically informed, and collaborative manner.\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge \n",
    "\n",
    "Define the following machine learning terms:\n",
    "\n",
    "supervised =  \n",
    "\n",
    "unsupervised =  \n",
    "\n",
    "classification =  \n",
    "\n",
    "regression =  \n",
    "\n",
    "dependent variable =  \n",
    "\n",
    "independent variable = \n",
    "\n",
    "performance metric =  \n",
    "\n",
    "data split =  \n",
    "\n",
    "training data =  \n",
    "\n",
    "test data =  \n",
    "\n",
    "cross-validation =  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**In the next lesson we will check out [term frequency–inverse document frequency](https://en.wikipedia.org/wiki/Tf%E2%80%93idf) - another type of document encoding that identifies unique words in text and documents. **"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
