{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# An introduction to computational text analysis: A more in-depth look at strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from string import punctuation\n",
    "punctuation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Strings have some simple but powerful methods that allow us to begin working with text in more complex ways. You saw how to import a .csv as a `datascience` Table in the last notebook, but what happens when we want to import text that is not nicely organized into rows and columns? We can organize the important parts into a nice tabular structure by first identifying parts that we want. \n",
    "\n",
    "**NOTE:** We will use hand-typed and plain text (.txt) file examples in this notebook, and since the rest of the class will focus on HathiTrust Research Center resources, **know that  .html, .json, and .xml file formats are important to computational text analysis but will not be covered in this class.** "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 1\n",
    "\n",
    "1. Store your first name in a variable named `first`.  \n",
    "2. Store your last name in a variable named `last`.  \n",
    "3. Convert `first` to all upper case letters.  \n",
    "4. Convert `last` to all lower case letters.  \n",
    "5. Combine these two string variables into a variable named `full`.  \n",
    "6. Slice out `first` from `full`.  \n",
    "7. Slice out `last` from `full`.  \n",
    "8. Slice `full` so that it contains only the last 2 characters of your first name and the first two characters of your last name.  \n",
    "9. Which string methods did you use in #3 and #4 above? How do you know they are string methods?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Bonus\n",
    "What do the following commands return? Why? \n",
    "\n",
    "\"cat\" > \"category\"  \n",
    "\"cat \" * 5  \n",
    "\"cat\" + 2  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jorge Luis Borges\n",
    "\n",
    "![borges](img/borges_1921.jpg)\n",
    "\n",
    "Below is a string of [Jorge Luis Borges'](https://en.wikipedia.org/wiki/Jorge_Luis_Borges) poem \"On His Blindness\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "borges = '''In the fullness of the years, like it or not,\n",
    "a luminous mist surrounds me, unvarying, \n",
    "that breaks things down into a single thing,\n",
    "colorless, formless. Almost into a thought. \n",
    "The elemental, vast night and the day\n",
    "teeming with people have become that fog\n",
    "of constant, tentative light that does not flag,\n",
    "and lies in wait at dawn. I longed to see\n",
    "just once a human face. Unknown to me\n",
    "the closed encyclopedia, the sweet play\n",
    "in volumes I can do no more than hold, \n",
    "the tiny soaring birds, the moons of gold.\n",
    "Others have the world, for better or worse; \n",
    "I have this half-dark, and the toil of verse.'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(borges)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# make an unpreprocessed copy for use below\n",
    "borges_dirty = borges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization\n",
    "Tokenization is the process of splitting text into words - each word is called a \"token\" and each word has a particular \"type\". However, a word such as \"the\" might adhere to multiple tokens of \"the\" within a text.\n",
    "\n",
    "`.split` allows us to split the text based on some sort of separator. In this case, we want to split on the \"whitespace\" (the blank spaces between words). \n",
    "\n",
    "**NOTE:** remember to use your help files in the form of `help(borges.split)`\n",
    "\n",
    "Let's just look at the first six words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borges.split()[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many characters are there in `borges`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(borges))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many words?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(borges.split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many lines? (hint: a line break is represented as \\n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(borges.split(\"\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many stanzas?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(borges.split(\"\\n\\n\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At which index does the word \"me\" first appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print(borges.find(\"me\")) # .find is \"forward search\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At which index does the word \"me\" last appear?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(borges.rfind(\"me\")) # .rfind starts at the highest index and works in reverse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "help(borges.rfind)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How many unique words? (hint, use `set`!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(set(borges.lower().split()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(set(borges.lower().split()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 2\n",
    "1. Using your intutition, how might you split text on commas?\n",
    "2. On periods?\n",
    "3. How do you split _all_ of `borges` on whitespace so that all words are split and printed?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some fast notes about for-loops, functions, and conditionals in Python\n",
    "\n",
    "Custom functions, for-loops, and conditionals are important tools that you will want to eventually explore. Since we will use a loop to remove punctuation below, let's take a minute to talk about these important topics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom functions\n",
    "\n",
    "Just like built-in functions, custom functions take some inputs and give you back desired output(s). We can define our own custom functions to use over and over again. \n",
    "\n",
    "In this case, `def` tells Python that we want to define our own function. `square` is the name of the function and it needs two arguments to work `x` and `y`. \n",
    "\n",
    "The colon symbol `:` tells Python that the code to be evaluated comes on the indented line after it.  \n",
    "\n",
    "`return` tells Python that the code after it should be printed out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sq_and_div(x,y):\n",
    "    return (x**2)/y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sq_and_div(5,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### For loops\n",
    "\n",
    "For loops are useful when you want to use the same code over a range of values, data, or files. \n",
    "\n",
    "`for` tells Python that we want to write a for loop. \n",
    "\n",
    "`x` is our \"iterator\" (placeholder) variable and range is the number of times to iterate. The colon symbol `:` again tells Python that the code to be evaluated follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in range(1, 13):\n",
    "    print(\"The time is\", x, \"o'clock\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditionals\n",
    "\n",
    "Conditionals are statements that help you assign different conditions to different pieces of data. In the case below, `if` tells Python that \"if some condition is met - do _this_\". \n",
    "\n",
    "However, \"if _some other condition_ is met - do something _else!_\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = int(input(\"What time is it (PM)?\"))\n",
    "if x < 9:\n",
    "    print(\"The time is\", x, \"o'clock\")\n",
    "else:\n",
    "    print(\"It's getting late!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing punctuation\n",
    "\n",
    "Remember how we imported that nice string of English punctuation in the first cell of this notebook? We could manually remove all of the punctuation using the `.replace` method, but this would get old fast!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(punctuation))\n",
    "print(punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "borges_periods = borges.replace(\".\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(borges_periods) # all periods have been successfully removed! "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But, what if you have tons of text and don't know exactly what punctuation is present? A quick custom function can help us remove all the punctuation from `borges`, i.e. !\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for char in punctuation:\n",
    "    borges = borges.replace(char, \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(borges)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Challenge 3\n",
    "\n",
    "Describe what is happening in this remove punctuation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for char in punctuation:\n",
    "    borges = borges.replace(char, \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenization with the `nltk` library\n",
    "\n",
    "The [`nltk` (natural language toolkit)](https://nltk.readthedocs.io/en/latest/) library can also help you tokenize your text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "tokens = word_tokenize(borges)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sentence segmentation\n",
    "\n",
    "Sentence segmentation deals with identifying sentence boundaries. We can do this by splitting on punctuation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borges_dirty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borges_dirty.split(\".\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `nltk` can do sentence segmentation also!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize\n",
    "sent_tokenize(borges_dirty)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do this with the hopes of \"normalizing\" our text. There are many scenarios that make text non-normalized, but some common ones include:\n",
    "- case folding (dealing with upper and lower case letters; generally, we want to make all text lower-case).\n",
    "- removing URLs, digits, and hashtags\n",
    "- infrequent word removal\n",
    "- stop word removal\n",
    "\n",
    "Regular expressions help out greatly with these! See me during office hours if you have further questions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Count word frequencies\n",
    "\n",
    "We can use Python's built-in function `Counter` to count words! Let's look at the most frequent twelve:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "freq = Counter(tokens)\n",
    "freq.most_common(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing stop words\n",
    "\n",
    "Yikes! The most common words in `borges` seem to be [stop words](https://en.wikipedia.org/wiki/Stop_words) such as \"the\", \"of\", and \"a\". Let's remove them because they are rarely useful in computational text analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stop = stopwords.words(\"english\")\n",
    "stop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_stops = [word for word in tokens if word not in stopwords.words('english')]\n",
    "no_stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stemming/lemmatization\n",
    "\n",
    "Both of these terms seek to remove morphological affixes on words. \n",
    "\n",
    "If we stem the word \"eats\" we get \"eat\". If we stem the word \"sleeping\" we get \"sleep\". We stem words because we tend to focus more on the meaning of the core content of the word, rather than its tense. \n",
    "\n",
    "NLTK provides many algorithms for stemming. For English, a great baseline is the [Porter](https://github.com/nltk/nltk/blob/develop/nltk/stem/porter.py) algorithm, which is in spirit isn't that far from a bunch of regular expressions.\n",
    "\n",
    "Let's try a few! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"eats\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"sleeping\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer.stem(\"flying\") # uh oh..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer, WordNetLemmatizer\n",
    "snowballer_stemmer = SnowballStemmer('english')\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(snowballer_stemmer.stem(\"eats\"))\n",
    "print(snowballer_stemmer.stem(\"sleeping\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(lemmatizer.lemmatize(\"leaves\")) # uh-oh..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part of speech tagging\n",
    "\n",
    "Part of speech (POS) tagging assigns each token a part of speech! (i.e., noun, verg, adjective, etc.). \n",
    "\n",
    "Again, there are many different [alternatives](https://github.com/nltk/nltk/tree/develop/nltk/tag), but NLTK keeps its recommended POS tagger available through the function `pos_tag`. The tagger expects a list of tokens as input. When doing POS tagging, it is advisable **not** to remove stop words beforehand (although you are free to do it afterwards)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Whoops! We forgot to remove our line breaks. Let's do so now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "borges = borges.replace(\"\\n\", \" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "borges # looking good! :) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import pos_tag\n",
    "pos_borges = borges\n",
    "pos_borges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tagged_borges = pos_tag(tokens)\n",
    "tagged_borges"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What might you conclude about Borges' style of writing based on the frequencies of non-stop words and stemmed words? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Why is preprocessing important?\n",
    "\n",
    "Text preprocessing is an essential first step to coding and understanding machine learning algorithms. For machine learning portions of this course, we will focus on bag of words models, namely document-term and term frequency-inverse document frequency models from the [sklearn library](http://scikit-learn.org/stable/). \n",
    "\n",
    "As previously stated, these instructions can be improved upon using regular expressions. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Challenge 4\n",
    "\n",
    "We can also open data from files. Let's open up the \"poe.txt\" file from the materials you downloaded earlier. This contains the poem \"A Dream Within a Dream\" by Edgar Allen Poe. \n",
    "\n",
    "Repeat the instructions in this notebook using Poe's poem. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(\"./poe.txt\", \"r\") as myfile:\n",
    "    poe = myfile.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(poe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(poe.split(\"\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## etc.\n",
    "\n",
    "## YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
